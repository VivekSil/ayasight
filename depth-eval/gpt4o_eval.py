# -*- coding: utf-8 -*-
"""gpt4o-eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K_goLBYfchflVt8Dh5QHt_sKNQRAXpnN
"""

# Install latest OpenAI client if needed
!pip install --upgrade --quiet openai datasets pandas pillow tqdm

from openai import OpenAI
from datasets import load_dataset
from PIL import Image
import pandas as pd
import base64
import io
from tqdm import tqdm

# Initialize client (your key must have GPT-4o access)
client = OpenAI(api_key="")  # Replace with your API key

# Load your dataset
dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples")["data"]

# Convert image to base64
def encode_image(img: Image.Image) -> str:
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode()

# Call GPT-4o with 1 or 2 images and a question
def ask_gpt(images, question):
    content = [{"type": "text", "text": question}]
    for img in reversed(images):  # Insert images at front
        content.insert(0, {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/png;base64,{encode_image(img)}"
            }
        })
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": content}],
            max_tokens=100,
            temperature=0.2,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ERROR: {e}"

# Prepare result lists
image_only_results = []
depth_aware_results = []

# Evaluate each sample
for sample in tqdm(dataset, desc="Evaluating with and without depth"):
    img = sample['image']
    depth = sample['depth_map']

    q1, a1 = sample['question_1'], sample['answer_1']
    q2, a2 = sample['question_2'], sample['answer_2']
    q3, a3 = sample['question_3'], sample['answer_3']

    # Image-only mode
    r1 = ask_gpt([img], q1)
    r2 = ask_gpt([img], q2)
    r3 = ask_gpt([img], q3)

    # Depth-aware mode (image + depth map)
    r1d = ask_gpt([img, depth], q1)
    r2d = ask_gpt([img, depth], q2)
    r3d = ask_gpt([img, depth], q3)

    image_only_results.append({
        "question_1": q1, "answer_1": a1, "response_1": r1,
        "question_2": q2, "answer_2": a2, "response_2": r2,
        "question_3": q3, "answer_3": a3, "response_3": r3,
    })

    depth_aware_results.append({
        "question_1": q1, "answer_1": a1, "response_1": r1d,
        "question_2": q2, "answer_2": a2, "response_2": r2d,
        "question_3": q3, "answer_3": a3, "response_3": r3d,
    })

# Save both CSVs
pd.DataFrame(image_only_results).to_csv("image_only_results.csv", index=False)
pd.DataFrame(depth_aware_results).to_csv("depth_aware_results.csv", index=False)

print("âœ… Saved:")
print("- image_only_results.csv")
print("- depth_aware_results.csv")

def ask_gpt_with_depth(rgb_img, depth_img, question):
    content = [
        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encode_image(depth_img)}"}},
        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encode_image(rgb_img)}"}},
        {"type": "text", "text": f"The first image is the depth map and the second image is the original scene. Use both to answer: {question}"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": content}],
            max_tokens=100,
            temperature=0.2,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ERROR: {e}"

depth_aware_results = []

# Evaluate each sample
for sample in tqdm(dataset, desc="Evaluating with and without depth"):
    img = sample['image']
    depth = sample['depth_map']

    q1, a1 = sample['question_1'], sample['answer_1']
    q2, a2 = sample['question_2'], sample['answer_2']
    q3, a3 = sample['question_3'], sample['answer_3']

    r1d = ask_gpt_with_depth(img, depth, q1)
    r2d = ask_gpt_with_depth(img, depth, q2)
    r3d = ask_gpt_with_depth(img, depth, q3)


    depth_aware_results.append({
        "question_1": q1, "answer_1": a1, "response_1": r1d,
        "question_2": q2, "answer_2": a2, "response_2": r2d,
        "question_3": q3, "answer_3": a3, "response_3": r3d,
    })

pd.DataFrame(depth_aware_results).to_csv("results-gpt40-depth-aware.csv", index=False)
