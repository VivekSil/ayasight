# -*- coding: utf-8 -*-
"""aya-depth-eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-UvZduEscn-mfkwtk2AuIklqNDkWJCro
"""

# @title Aya Vision 8B Evaluation on Depth-Aware VQA Dataset
!pip install -q 'git+https://github.com/huggingface/transformers.git@v4.49.0-AyaVision' datasets torchvision

!pip install -q huggingface_hub

from huggingface_hub import login
login("")

from transformers import AutoProcessor, AutoModelForImageTextToText
from datasets import load_dataset
from PIL import Image
import torch
from tqdm import tqdm
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import torchvision.transforms as T

# Load Aya Vision 8B model
model_id = "CohereLabs/aya-vision-8b"
processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForImageTextToText.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")

# Load your dataset
dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples")['data']

dataset[0]

import tempfile
import base64
def encode_image_to_base64(pil_img):
    with tempfile.NamedTemporaryFile(suffix=".jpg") as tmp:
        pil_img.save(tmp.name)
        with open(tmp.name, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")

from difflib import SequenceMatcher
def similarity(a, b):
    return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()

# Binary QA: Yes/No
def evaluate_binary_qa(sample):
    prompt = sample["question_1"]
    target = sample["answer_1"]
    b64_image = encode_image_to_base64(sample["image"])

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "text", "text": prompt}
        ]
    }]

    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=128)
    pred = processor.decode(outputs[0], skip_special_tokens=True)

    return pred.strip(), pred.strip().lower() == target.strip().lower()

# Short answer QA
def evaluate_short_answer(sample):
    prompt = sample["question_2"]
    target = sample["answer_2"]
    b64_image = encode_image_to_base64(sample["image"])

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "text", "text": prompt}
        ]
    }]

    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=128)
    pred = processor.decode(outputs[0], skip_special_tokens=True)

    return pred.strip(), similarity(pred, target)

# List-based depth QA
from IPython.display import display

# List-based depth QA
def evaluate_list_depth_qa(sample):
    prompt = sample["question_3"]
    target = sample["answer_3"]
    b64_image = encode_image_to_base64(sample["image"])

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "text", "text": prompt}
        ]
    }]

    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=256)
    pred = processor.decode(outputs[0], skip_special_tokens=True)

    return pred.strip(), similarity(pred, target)

'''import pandas as pd
from tqdm import tqdm

# Cleaning function to remove Aya preambles and extract only the response
def clean_prediction(text):
    if isinstance(text, str):
        if "<|CHATBOT_TOKEN|>" in text:
            return text.split("<|CHATBOT_TOKEN|>")[-1].strip()
        else:
            # Fallback: try to extract the last sentence or meaningful chunk
            stripped = text.strip().split("\n")
            return stripped[-1].strip() if stripped else text.strip()
    return text

records = []

for sample in tqdm(dataset, desc="Evaluating all samples"):
    try:
        # Run all 3 QA evaluations
        pred1, acc1 = evaluate_binary_qa(sample)
        pred2, sim2 = evaluate_short_answer(sample)
        pred3, sim3 = evaluate_list_depth_qa(sample)

        # Clean all predictions
        clean_pred1 = clean_prediction(pred1)
        clean_pred2 = clean_prediction(pred2)
        clean_pred3 = clean_prediction(pred3)

        # Store full row
        records.append({
            "question_1": sample["question_1"],
            "answer_1": sample["answer_1"],
            "prediction_1": clean_pred1,
            "accuracy_1": acc1,

            "question_2": sample["question_2"],
            "answer_2": sample["answer_2"],
            "prediction_2": clean_pred2,
            "similarity_2": sim2,

            "question_3": sample["question_3"],
            "answer_3": sample["answer_3"],
            "prediction_3": clean_pred3,
            "similarity_3": sim3,
        })
    except Exception as e:
        print(f"⚠️ Error at sample {sample.get('sample_id', 'unknown')}: {e}")

# Save to DataFrame
df_results = pd.DataFrame(records)'''

#df_results.head()

#convert to csv
#df_results.to_csv('results-aya-vision-no-depth.csv', index=False)

'''from google.colab import drive
drive.mount('/content/drive')'''

'''from datasets import load_dataset
from PIL import Image
import os, zipfile

# Load your dataset
dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples", split="data")

# Create folders
os.makedirs("rgb_images", exist_ok=True)
os.makedirs("depth_maps", exist_ok=True)

# Save images
for i, sample in enumerate(dataset):
    sample["image"].save(f"rgb_images/image_{i}.png")
    sample["depth_map"].save(f"depth_maps/depth_{i}.png")

# Zip RGB images
with zipfile.ZipFile("rgb_images.zip", "w") as zipf:
    for fname in os.listdir("rgb_images"):
        zipf.write(os.path.join("rgb_images", fname), arcname=fname)

# Zip depth maps
with zipfile.ZipFile("depth_maps.zip", "w") as zipf:
    for fname in os.listdir("depth_maps"):
        zipf.write(os.path.join("depth_maps", fname), arcname=fname)

print("✅ Zipped images and depth maps. Use file browser on the left to download.")'''

def encode_dual_images(rgb_image, depth_image):
    """Returns two base64-encoded image dicts for chat-style prompting."""
    return [
        {"type": "image", "image": encode_image_to_base64(rgb_image)},
        {"type": "image", "image": encode_image_to_base64(depth_image)}
    ]

def evaluate_binary_qa_with_depth(sample):
    prompt = sample["question_1"]
    target = sample["answer_1"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=64)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    #clean = extract_clean_answer(pred)

    acc = int(pred.lower() == target.lower())
    return pred, acc

def evaluate_short_answer_with_depth(sample):
    prompt = sample["question_2"]
    target = sample["answer_2"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=128)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    return pred, similarity(pred, target)

def evaluate_list_depth_qa_with_depth(sample):
    prompt = sample["question_3"]
    target = sample["answer_3"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=256)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    return pred, similarity(pred, target)

from tqdm import tqdm
import pandas as pd

def clean_prediction(text):
    if isinstance(text, str):
        if "<|CHATBOT_TOKEN|>" in text:
            return text.split("<|CHATBOT_TOKEN|>")[-1].strip()
        else:
            # Fallback: try to extract the last sentence or meaningful chunk
            stripped = text.strip().split("\n")
            return stripped[-1].strip() if stripped else text.strip()
    return text

records_with_depth = []

for sample in tqdm(dataset, desc="Evaluating with Depth Map"):
    pred1, acc1 = evaluate_binary_qa_with_depth(sample)
    pred2, sim2 = evaluate_short_answer_with_depth(sample)
    pred3, sim3 = evaluate_list_depth_qa_with_depth(sample)

    # Clean all predictions
    clean_pred1 = clean_prediction(pred1)
    clean_pred2 = clean_prediction(pred2)
    clean_pred3 = clean_prediction(pred3)

    records_with_depth.append({
        "question_1": sample["question_1"],
        "answer_1": sample["answer_1"],
        "prediction_1": clean_pred1,
        "accuracy_1": acc1,
        "question_2": sample["question_2"],
        "answer_2": sample["answer_2"],
        "prediction_2": clean_pred2,
        "similarity_2": sim2,
        "question_3": sample["question_3"],
        "answer_3": sample["answer_3"],
        "prediction_3": clean_pred3,
        "similarity_3": sim3,
    })

df_with_depth = pd.DataFrame(records_with_depth)
df_with_depth.to_csv("depth_aware_evaluation_results.csv", index=False)
