# -*- coding: utf-8 -*-
"""aya-depth-eval-pt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U8z5I4_jW_Ifte8PHpLSp0P_c8pwdjsQ
"""

# @title Aya Vision 8B Evaluation on Depth-Aware VQA Dataset
!pip install -q 'git+https://github.com/huggingface/transformers.git@v4.49.0-AyaVision' datasets torchvision

!pip install -q huggingface_hub

from huggingface_hub import login
login("")

from transformers import AutoProcessor, AutoModelForImageTextToText
from datasets import load_dataset
from PIL import Image
import torch
from tqdm import tqdm
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import torchvision.transforms as T

# Load Aya Vision 8B model
model_id = "CohereLabs/aya-vision-8b"
processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForImageTextToText.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")

# Load your dataset
dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples")['data']

import tempfile
import base64
def encode_image_to_base64(pil_img):
    with tempfile.NamedTemporaryFile(suffix=".jpg") as tmp:
        pil_img.save(tmp.name)
        with open(tmp.name, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")

def encode_dual_images(rgb_image, depth_image):
    """Returns two base64-encoded image dicts for chat-style prompting."""
    return [
        {"type": "image", "image": encode_image_to_base64(rgb_image)},
        {"type": "image", "image": encode_image_to_base64(depth_image)}
    ]

from difflib import SequenceMatcher
def similarity(a, b):
    return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()

def evaluate_binary_qa_with_depth(sample):
    prompt = sample["question_1"]
    target = sample["answer_1"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=64)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    #clean = extract_clean_answer(pred)

    acc = int(pred.lower() == target.lower())
    return pred, acc

def evaluate_short_answer_with_depth(sample):
    prompt = sample["question_2"]
    target = sample["answer_2"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=128)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    return pred, similarity(pred, target)

torch.backends.cuda.enable_flash_sdp(False)
torch.backends.cuda.enable_math_sdp(True)
torch.backends.cuda.enable_mem_efficient_sdp(False)

def evaluate_list_depth_qa_with_depth(sample):
    prompt = sample["question_3"]
    target = sample["answer_3"]
    image = sample["image"]
    depth = sample["depth_map"]

    b64_image = encode_image_to_base64(image)
    b64_depth = encode_image_to_base64(depth)

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": b64_image},
            {"type": "image", "image": b64_depth},
            {"type": "text", "text": prompt}
        ]
    }]
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=False, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(inputs, max_new_tokens=256)
    pred = processor.decode(outputs[0], skip_special_tokens=True).strip()
    return pred, similarity(pred, target)

from tqdm import tqdm
import pandas as pd

def clean_prediction(text):
    if isinstance(text, str):
        if "<|CHATBOT_TOKEN|>" in text:
            return text.split("<|CHATBOT_TOKEN|>")[-1].strip()
        else:
            # Fallback: try to extract the last sentence or meaningful chunk
            stripped = text.strip().split("\n")
            return stripped[-1].strip() if stripped else text.strip()
    return text

records_with_depth = []

for sample in tqdm(dataset, desc="Evaluating with Depth Map"):
    pred1, acc1 = evaluate_binary_qa_with_depth(sample)
    pred2, sim2 = evaluate_short_answer_with_depth(sample)
    pred3, sim3 = evaluate_list_depth_qa_with_depth(sample)

    # Clean all predictions
    clean_pred1 = clean_prediction(pred1)
    clean_pred2 = clean_prediction(pred2)
    clean_pred3 = clean_prediction(pred3)

    records_with_depth.append({
        "question_1": sample["question_1"],
        "answer_1": sample["answer_1"],
        "prediction_1": clean_pred1,
        "accuracy_1": acc1,
        "question_2": sample["question_2"],
        "answer_2": sample["answer_2"],
        "prediction_2": clean_pred2,
        "similarity_2": sim2,
        "question_3": sample["question_3"],
        "answer_3": sample["answer_3"],
        "prediction_3": clean_pred3,
        "similarity_3": sim3,
    })

df_with_depth = pd.DataFrame(records_with_depth)
df_with_depth.to_csv("depth_aware_evaluation_results.csv", index=False)

df_with_depth.head()

df_with_depth.iloc[0][:]
