# -*- coding: utf-8 -*-
"""qwen-depth-eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajvVsJwxNYQSEcrIILjKNWtTubTfilmc
"""

!pip install -q transformers accelerate datasets torchvision Pillow
!pip install -q vllm decord huggingface_hub

from vllm import LLM, SamplingParams
from huggingface_hub import hf_hub_download
from PIL import Image
from decord import VideoReader, cpu
import base64
import io

from huggingface_hub import login
login("")

llm = LLM("Qwen/Qwen2.5-VL-3B-Instruct", trust_remote_code=True)

def encode_image(image):
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    image_bytes = buffered.getvalue()
    return base64.b64encode(image_bytes).decode("utf-8")

dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples")["data"]

def run_qwen_with_images(images, prompt_text):
    # Initialize Qwen model in vLLM (make sure GPU is available)


    # Format message
    messages = [
        {
            "role": "user",
            "content": [{"type": "text", "text": prompt_text}] + [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{encode_image(img)}"}
                } for img in images
            ]
        }
    ]

    # Generation parameters
    sampling_params = SamplingParams(max_tokens=150)
    outputs = llm.chat(messages, sampling_params)
    return outputs[0]["generation"]

# ✅ Load your dataset
dataset = load_dataset("srimoyee12/openspaces-depth-aware-32-samples")["data"]

# ✅ Evaluation function (image only OR image+depth)
def evaluate_dataset(dataset, use_depth=False):
    rows = []

    for sample in tqdm(dataset, desc=f"Evaluating with {'Image+Depth' if use_depth else 'Image Only'}"):
        image = sample["image"]
        depth = sample["depth_map"]
        questions = [sample["question_1"], sample["question_2"], sample["question_3"]]
        answers = [sample["answer_1"], sample["answer_2"], sample["answer_3"]]
        responses = []

        for q in questions:
            input_images = [image, depth] if use_depth else [image]
            try:
                response = run_qwen_with_images(input_images, prompt_text=q)
            except Exception as e:
                print(f"❌ Error with sample {sample.get('sample_id')}, Q: {q} → {e}")
                response = "ERROR"
            responses.append(response)

        rows.append({
            "question_1": questions[0], "answer_1": answers[0], "response_1": responses[0],
            "question_2": questions[1], "answer_2": answers[1], "response_2": responses[1],
            "question_3": questions[2], "answer_3": answers[2], "response_3": responses[2],
        })

    return pd.DataFrame(rows)

df_img_only = evaluate_dataset(dataset, use_depth=False)
df_img_depth = evaluate_dataset(dataset, use_depth=True)

# ✅ Save to CSV
df_img_only.to_csv("qwen_img_only_results.csv", index=False)
df_img_depth.to_csv("qwen_img_depth_results.csv", index=False)

# ✅ Preview
df_img_only.head()



